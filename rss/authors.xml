<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>authors</title>
    <link>https://raw.githubusercontent.com/jkitchin/literature-alerts/main/rss/rss/authors.xml</link>
    <description>List of authors</description>
    <language>en-US</language>
    <lastBuildDate>Tue, 14 May 2024 01:05:23 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>CatTSunami: Accelerating Transition State Energy Calculations with
  Pre-trained Graph Neural Networks</title>
      <link>https://doi.org/10.48550/arxiv.2405.02078</link>
      <description>Brook Wander, Muhammed Shuaibi, John R. Kitchin, Zachary W. Ulissi, C. Lawrence Zitnick, arXiv (Cornell University). None(None)] 2024. https://openalex.org/W4396781988

                Direct access to transition state energies at low computational cost unlocks the possibility of accelerating catalyst discovery. We show that the top performing graph neural network potential trained on the OC20 dataset, a related but different task, is able to find transition states energetically similar (within 0.1 eV) to density functional theory (DFT) 91% of the time with a 28x speedup. This speaks to the generalizability of the models, having never been explicitly trained on reactions, the machine learned potential approximates the potential energy surface well enough to be performant for this auxiliary task. We introduce the Open Catalyst 2020 Nudged Elastic Band (OC20NEB) dataset, which is made of 932 DFT nudged elastic band calculations, to benchmark machine learned model performance on transition state energies. To demonstrate the efficacy of this approach, we replicated a well-known, large reaction network with 61 intermediates and 174 dissociation reactions at DFT resolution (40 meV). In this case of dense NEB enumeration, we realize even more computational cost savings and used just 12 GPU days of compute, where DFT would have taken 52 GPU years, a 1500x speedup. Similar searches for complete reaction networks could become routine using the approach presented here. Finally, we replicated an ammonia synthesis activity volcano and systematically found lower energy configurations of the transition states and intermediates on six stepped unary surfaces. This scalable approach offers a more complete treatment of configurational space to improve and accelerate catalyst discovery.</description>
      <author>Brook Wander, Muhammed Shuaibi, John R. Kitchin, Zachary W. Ulissi, C. Lawrence Zitnick</author>
      <pubDate>Fri, 03 May 2024 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://doi.org/10.48550/arxiv.2405.02078</guid>
    </item>
    <item>
      <title>AdsorbDiff: Adsorbate Placement via Conditional Denoising Diffusion</title>
      <link>https://doi.org/10.48550/arxiv.2405.03962</link>
      <description>Adeesh Kolluru, John R. Kitchin, arXiv (Cornell University). None(None)] 2024. https://openalex.org/W4396813915

                Determining the optimal configuration of adsorbates on a slab (adslab) is pivotal in the exploration of novel catalysts across diverse applications. Traditionally, the quest for the lowest energy adslab configuration involves placing the adsorbate onto the slab followed by an optimization process. Prior methodologies have relied on heuristics, problem-specific intuitions, or brute-force approaches to guide adsorbate placement. In this work, we propose a novel framework for adsorbate placement using denoising diffusion. The model is designed to predict the optimal adsorbate site and orientation corresponding to the lowest energy configuration. Further, we have an end-to-end evaluation framework where diffusion-predicted adslab configuration is optimized with a pretrained machine learning force field and finally evaluated with Density Functional Theory (DFT). Our findings demonstrate an acceleration of up to 5x or 3.5x improvement in accuracy compared to the previous best approach. Given the novelty of this framework and application, we provide insights into the impact of pre-training, model architectures, and conduct extensive experiments to underscore the significance of this approach.</description>
      <author>Adeesh Kolluru, John R. Kitchin</author>
      <pubDate>Mon, 06 May 2024 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://doi.org/10.48550/arxiv.2405.03962</guid>
    </item>
  </channel>
</rss>
