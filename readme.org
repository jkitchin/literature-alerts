#+title: Literature alerts

Eventually this will do something on a schedule to pull new alerts from openalex.org

See results in [[./results.org]]

But first:

* Run an action on a schedule

See [[./.github/workflows/scheduled.yml]]. Adapted from https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#schedule

This runs every 15 min. That might be the smallest interval
#+BEGIN_EXAMPLE
on:
  schedule:
    - cron: '*/15 * * * *'
#+END_EXAMPLE


* Have an action create an artifact

probably a file, and maybe also a GH issue

Go to https://github.com/jkitchin/literature-alerts/settings/actions and give actions read/write permissions at the bottom.

You can commit results in an action. The downside of this is you have to pull before you can push again. That is probably ok

#+BEGIN_EXAMPLE
  build_artifact:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Create a file
        run: |
          date >> results.dat
          git config --global user.email "jkitchin@andrew.cmu.edu"
          git config --global user.name "John Kitchin"
          git add results.dat
          git commit results.dat -m "adding to results.dat"
          git push
#+END_EXAMPLE


* Use a GH secret to save the API key

The api key is secret, and you add it to an environment like this. Then in the script.py load it from the environment.

#+BEGIN_EXAMPLE
  openalex:
    runs-on: ubuntu-latest
    steps:
      - name: Install dependencies
        run: pip install requests
        
      - name: Use API key
        env:
          OPENALEX_API_KEY: ${{ secrets.OPENALEX_API_KEY }}
        run: |            
            python script.py
            git add results.dat
            git commit results.dat -m "adding new results to results.dat"
            git push
#+END_EXAMPLE



* write a Python script using OpenAlex to get new articles

Locally I have a .env file (not part of the repository), and I load it here.

#+BEGIN_SRC jupyter-python
from dotenv import load_dotenv

load_dotenv()
#+END_SRC

#+RESULTS:
:RESULTS:
True
:END:

This script gets new items from the last six weeks. Eventually I need to make this be aligned with the scheduled frequency, but it gets a paper for testing.



#+BEGIN_SRC jupyter-python :tangle script.py :shebang #!/usr/bin/env python
import os
import sys
import requests
import datetime
from math import ceil

API_KEY = os.environ['OPENALEX_API_KEY']

today = datetime.date.today()
day_ago = (today - datetime.timedelta(days=7)).strftime("%Y-%m-%d")

url = f'https://api.openalex.org/works?filter=title_and_abstract.search:oxygen+evolution,from_created_date:{day_ago}&api_key={API_KEY}'

data = requests.get(url).json()

count = data['meta']['count']
perpage = data['meta']['per_page']
npages = ceil(count / perpage)

def process_result(result):
    authors = ', '.join([au['author']['display_name'] for au in result['authorships'] ])
    word_index = []

    aii = result.get('abstract_inverted_index', None)

    if aii:
        for k,v in aii.items():
            for index in v:
                word_index.append([k, index])

        word_index = sorted(word_index,key = lambda x : x[1])
        abstract = ' '.join([x[0] for x in word_index])
    else:
        abstract = 'No abstract'

    source = result.get('primary_location', {}).get('source', {})
    if source:
        host = source.get('display_name', 'No host')
    else:
        host = 'No host'

    return f'''** {result['title']}
:PROPERTIES:
:ID: {result['id']}
:DOI: {result['doi']}
:AUTHORS: {authors}
:HOST: {host}
:END:

    
[[elisp:(doi-add-bibtex-entry "{result['doi']}")][Get bibtex entry]]

- [[elisp:(progn (xref--push-markers (current-buffer) (point)) (oa--referenced-works "{result['id']}"))][Get references]]
- [[elisp:(progn (xref--push-markers (current-buffer) (point)) (oa--related-works "{result['id']}"))][Get related work]]
- [[elisp:(progn (xref--push-markers (current-buffer) (point)) (oa--cited-by-works "{result['id']}"))][Get cited by]]

{abstract}    

    
'''

# Process page 1
s = ''
for result in data['results']:
    s += process_result(result)

for i in range(1, npages):
    purl = url + f'&page={i}'
    data = requests.get(url).json()
    for result in data['results']:
        s += process_result(result)
  
    
with open('results.org', 'w') as f:
    f.write(f'* Results for {day_ago}\n\n')
    f.write(s)


#+END_SRC

#+RESULTS:


* Create a new issue when new things are found

This would alert you that there is something to do.

https://github.com/marketplace/actions/create-an-issue


#+BEGIN_EXAMPLE
      - uses: JasonEtco/create-an-issue@v2        
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
#+END_EXAMPLE

there is not a lot of control, but it is ok.

It would be useful if it only did this when new entries are found. That would require some logic to see if the results.org file changed maybe, or some flag file.

It is possible it would be easier to do this in the script?

it is, I can do it with the gh cli.
